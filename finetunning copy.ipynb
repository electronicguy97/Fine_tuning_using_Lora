{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, DiffusionPipeline, AutoencoderKL\n",
    "from diffusers.utils import make_image_grid\n",
    "from diffusers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|특성|StableDiffusionPipeline|DiffusionPipeLine|\n",
    "|---|---|---|\n",
    "|설계 목적|Stable Diffusion에 특화된 작업 수행|범용 Diffusion 모델 관리 및 커스터마이징|\n",
    "|구성 요소|Stable Diffusion에 필요한 모든 구성요소 포함|사용자가 커스터마이징 가능|\n",
    "|사용 편의성|간단하게 텍스트-이미지 생성가능|직접 구성 요소를 설정해야 할 수도 있음|\n",
    "|적용 가능 모델|Stable Diffusion모델|다양한 Diffusion 모델|\n",
    "|커스터마이징|제한적(Stable Diffusion 구조 내에서만 가능)|완전히 사용자 정의 가능|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 사전 다운 -> huggingface 로그인해서 해도 됌\n",
    "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git lfs install\n",
    "#!git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\n",
    "\n",
    "#!huggingface-cli login # 로그인 필요 없음: 공개 모델 사용 시 생략 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # PyTorch 버전 확인\n",
    "print(torch.cuda.is_available())  # CUDA 사용 가능 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport cv2\\nimport numpy as np\\nfrom torchvision import transforms\\nfrom PIL import Image\\n\\ndef prepare_dataset(input_dir, output_dir, augment=False):\\n    \\'\\'\\'\\n    Prepare dataset with optional augmentation.\\n    \\'\\'\\'\\n    if not os.path.exists(output_dir):\\n        os.makedirs(output_dir)\\n    \\n    for label in [\"Winding\", \"Good\"]:\\n            class_dir = os.path.join(input_dir, label)\\n            output_class_dir = os.path.join(output_dir, label)\\n            if not os.path.exists(output_class_dir):\\n                os.makedirs(output_class_dir)\\n\\n            for file_name in os.listdir(class_dir):\\n                # Ensure the file is an image\\n                if not file_name.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.bmp\\')):\\n                    print(f\"Skipping non-image file: {file_name}\")\\n                    continue\\n\\n                file_path = os.path.join(class_dir, file_name)\\n                img = cv2.imread(file_path)\\n\\n                if img is None:\\n                    print(f\"Failed to load image: {file_path}\")\\n                    continue\\n\\n                # Save original image\\n                output_path = os.path.join(output_class_dir, file_name.replace(\".bmp\", \".jpg\"))\\n                #output_path = os.path.join(output_class_dir, file_name)\\n                cv2.imwrite(output_path, img)\\n\\n                if augment:\\n                    # Example augmentations: flip, rotate, noise\\n                    flipped = cv2.flip(img, 1)\\n                    rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\\n                    noisy = cv2.add(img, np.random.normal(0, 25, img.shape).astype(np.uint8))\\n                    \\n                    # Save augmented images\\n                    #cv2.imwrite(os.path.join(output_class_dir, f\"flipped_{file_name}\"), flipped)\\n                    #cv2.imwrite(os.path.join(output_class_dir, f\"rotated_{file_name}\"), rotated)\\n                    #cv2.imwrite(os.path.join(output_class_dir, f\"noisy_{file_name}\"), noisy)\\n                    cv2.imwrite(os.path.join(output_class_dir, f\"flipped_{file_name.replace(\\'.bmp\\', \\'.jpg\\')}\"), flipped, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\\n                    cv2.imwrite(os.path.join(output_class_dir, f\"rotated_{file_name.replace(\\'.bmp\\', \\'.jpg\\')}\"), rotated, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\\n                    cv2.imwrite(os.path.join(output_class_dir, f\"noisy_{file_name.replace(\\'.bmp\\', \\'.jpg\\')}\"), noisy, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\\n\\n\\n# Prepare dataset\\nprepare_dataset(\"D:/Data\", \"D:/edu/output\", augment=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 전처리 데이터 증강 데이터 많으면 필요없음\n",
    "\"\"\"import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def prepare_dataset(input_dir, output_dir, augment=False):\n",
    "    '''\n",
    "    Prepare dataset with optional augmentation.\n",
    "    '''\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for label in [\"Winding\", \"Good\"]:\n",
    "            class_dir = os.path.join(input_dir, label)\n",
    "            output_class_dir = os.path.join(output_dir, label)\n",
    "            if not os.path.exists(output_class_dir):\n",
    "                os.makedirs(output_class_dir)\n",
    "\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                # Ensure the file is an image\n",
    "                if not file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                    print(f\"Skipping non-image file: {file_name}\")\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                img = cv2.imread(file_path)\n",
    "\n",
    "                if img is None:\n",
    "                    print(f\"Failed to load image: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Save original image\n",
    "                output_path = os.path.join(output_class_dir, file_name.replace(\".bmp\", \".jpg\"))\n",
    "                #output_path = os.path.join(output_class_dir, file_name)\n",
    "                cv2.imwrite(output_path, img)\n",
    "\n",
    "                if augment:\n",
    "                    # Example augmentations: flip, rotate, noise\n",
    "                    flipped = cv2.flip(img, 1)\n",
    "                    rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "                    noisy = cv2.add(img, np.random.normal(0, 25, img.shape).astype(np.uint8))\n",
    "                    \n",
    "                    # Save augmented images\n",
    "                    #cv2.imwrite(os.path.join(output_class_dir, f\"flipped_{file_name}\"), flipped)\n",
    "                    #cv2.imwrite(os.path.join(output_class_dir, f\"rotated_{file_name}\"), rotated)\n",
    "                    #cv2.imwrite(os.path.join(output_class_dir, f\"noisy_{file_name}\"), noisy)\n",
    "                    cv2.imwrite(os.path.join(output_class_dir, f\"flipped_{file_name.replace('.bmp', '.jpg')}\"), flipped, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "                    cv2.imwrite(os.path.join(output_class_dir, f\"rotated_{file_name.replace('.bmp', '.jpg')}\"), rotated, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "                    cv2.imwrite(os.path.join(output_class_dir, f\"noisy_{file_name.replace('.bmp', '.jpg')}\"), noisy, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "prepare_dataset(\"D:/Data\", \"D:/edu/output\", augment=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8e8c426f914ee0a606fd79f88c6595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 파이프라인 로드\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"model/stable-diffusion-v1-5\",\n",
    "    #revision=\"fp16\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721d36524ee04f39a1280d5c1f9688b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 데이터셋 로드\n",
    "#dataset = load_dataset(\"imagefolder\", data_dir=\"D:/edu/output\", streaming=True)\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"D:/edu/output\")\n",
    "expected_size = pipe.unet.config.sample_size\n",
    "# 이미지 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((expected_size, expected_size)),  # 이미지 크기 조정 -> 그래픽 카드 사양에 따라 조정 높을 수록 정밀도 올라감\n",
    "    transforms.ToTensor(),          # 텐서로 변환\n",
    "    transforms.Normalize([0.5], [0.5])  # 정규화\n",
    "])\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_dataset(example):\n",
    "    \"\"\"\n",
    "    데이터셋의 이미지를 변환하는 함수.\n",
    "    \"\"\"\n",
    "    if isinstance(example[\"image\"], list):  # 이미지가 리스트로 제공되는 경우\n",
    "        # 리스트에 포함된 각 이미지에 대해 변환을 적용\n",
    "        transformed_images = [transform(image) for image in example[\"image\"]]\n",
    "        example[\"image\"] = torch.stack(transformed_images)  # 변환된 이미지들을 스택으로 쌓음\n",
    "    else:  # 이미지가 PIL.Image 형식으로 제공되는 경우\n",
    "        example[\"image\"] = transform(example[\"image\"])  # transform 적용\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "# map 메서드로 변환 적용\n",
    "#dataset = dataset.map(preprocess_dataset, batched=True, batch_size= 4) # 배치사이즈가 낮을 수록 메모리 사용 적음음\n",
    "dataset = dataset.map(preprocess_dataset, batched=False) # 배치사이즈가 낮을 수록 메모리 사용 적음음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn 함수: 배치 처리\n",
    "def collate_fn(batch):\n",
    "    # Ensure images are tensors\n",
    "    images = torch.stack([torch.tensor(item[\"image\"]).float() if not isinstance(item[\"image\"], torch.Tensor) else item[\"image\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "    return {\"image\": images, \"label\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset[\"train\"], batch_size=4, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(pipe, dataloader, epochs=5, learning_rate=5e-5):\n",
    "    \n",
    "    prediction_type = None # 'epsilon'  or 'v_prediction'\n",
    "    train_losses = []\n",
    "    \n",
    "    pipe.unet.train()  # UNet 학습 모드로 설정\n",
    "    #optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, pipe.unet.parameters()), lr=learning_rate)\n",
    "    \n",
    "    scaler = GradScaler()  # AMP용 GradScaler 생성\n",
    "\n",
    "    # 학습률 스케줄러 (옵션, 필요 시 사용)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            images = batch[\"image\"].to(\"cuda\").half()  # .half()를 사용하여 float16으로 변환\n",
    "            labels = batch[\"label\"]  # 라벨 (필요에 따라 사용)\n",
    "\n",
    "            # VAE를 통해 latent 공간으로 매핑\n",
    "            #latents = pipe.vae.encode(images).latent_dist.sample() * 0.18215  # Scale factor\n",
    "            latents = pipe.vae.encode(images).latent_dist.sample() * pipe.vae.config.scaling_factor  # Scale factor\n",
    "\n",
    "            # 노이즈 추가\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning, [0]: last_hidden_state\n",
    "            encoder_hidden_states = pipe.text_encoder(batch[\"input_ids\"].to('cuda') )[0]\n",
    "\n",
    "            # 노이즈 스케쥴러에 예측 타입이 노이즈인지 이미지인지에 따라 타겟 설정\n",
    "            if prediction_type is not None:\n",
    "                # set prediction_type of scheduler if defined\n",
    "                pipe.noise_scheduler.register_to_config(prediction_type=prediction_type)\n",
    "\n",
    "            if pipe.noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif pipe.noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = pipe.noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {pipe.noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                train_losses.append(loss.item())\n",
    "            '''\n",
    "            noisy_latents_resized = torch.nn.functional.interpolate(noisy_latents, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():  # 자동 혼합 정밀도 사용\n",
    "                noise_pred = pipe.unet(noisy_latents, timesteps, None).sample\n",
    "                loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # loss.backward() 시 AMP 스케일러 사용\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # 메모리 비우기 (필요한 경우)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            '''\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        get_scheduler('linear', optimizer= optimizer, num_warmup_steps=500, num_training_steps=5 * len(dataloader)).step()\n",
    "        optimizer.zero_grad()\n",
    "           \n",
    "        '''\n",
    "        # Epoch마다 출력\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Avg Loss: {total_loss / len(dataloader)}\")\n",
    "        '''\n",
    "        avg_loss = sum(train_losses[-100:])/100\n",
    "        print(f'Finished epoch {epoch+1}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "        # 학습률 스케줄러 업데이트 (옵션, 필요 시 사용)\n",
    "        scheduler.step()\n",
    "\n",
    "    # 학습된 가중치 저장\n",
    "    pipe.unet.save_pretrained(\"fine_tuned_model/unet\")\n",
    "    pipe.vae.save_pretrained(\"fine_tuned_model/vae\")\n",
    "    pipe.text_encoder.save_pretrained(\"fine_tuned_model/text_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "#!huggingface-cli login  # 로그인\n",
    "#!git lfs install        # Git LFS 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [320, 4, 3, 3], expected input[4, 768, 8, 8] to have 4 channels, but got 768 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 훈련\u001b[39;00m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fine-tuned 모델 저장\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pipe\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[74], line 42\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[1;34m(pipe, dataloader, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# 자동 혼합 정밀도 사용\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# loss.backward() 시 AMP 스케일러 사용\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1169\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1164\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_encoder_hidden_states(\n\u001b[0;32m   1165\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states, added_cond_kwargs\u001b[38;5;241m=\u001b[39madded_cond_kwargs\n\u001b[0;32m   1166\u001b[0m )\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m# 2. pre-process\u001b[39;00m\n\u001b[1;32m-> 1169\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;66;03m# 2.5 GLIGEN position net\u001b[39;00m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [320, 4, 3, 3], expected input[4, 768, 8, 8] to have 4 channels, but got 768 channels instead"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "fine_tune_model(pipe, dataloader)\n",
    "\n",
    "# Fine-tuned 모델 저장\n",
    "pipe.save_pretrained(\"fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습한 모델 추가 default : stable-diffusion v1.5\n",
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Make NG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt를 통해 이미지 1장 생성\n",
    "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "image = pipeline(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "image = pipeline(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(batch_size=1):\n",
    "    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n",
    "    prompts = batch_size * [prompt]\n",
    "    num_inference_steps = 20\n",
    "\n",
    "    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipeline(**get_inputs(batch_size=4)).images\n",
    "make_image_grid(images, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipeline(**get_inputs(batch_size=8)).images\n",
    "make_image_grid(images, rows=2, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.vae = vae\n",
    "images = pipeline(**get_inputs(batch_size=8)).images\n",
    "make_image_grid(images, rows=2, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promt 작성\n",
    "prompt += \", tribal panther make up, blue on red, side profile, looking away, serious eyes\"\n",
    "prompt += \" 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipeline(**get_inputs(batch_size=8)).images\n",
    "make_image_grid(images, rows=2, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 프롬프트를 참고하여 작성\n",
    "prompts = [\n",
    "    \"portrait photo of the oldest warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n",
    "    \"portrait photo of an old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n",
    "    \"portrait photo of a warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n",
    "    \"portrait photo of a young warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n",
    "]\n",
    "\n",
    "generator = [torch.Generator(\"cuda\").manual_seed(1) for _ in range(len(prompts))]\n",
    "images = pipeline(prompt=prompts, generator=generator, num_inference_steps=25).images\n",
    "make_image_grid(images, 2, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
